{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Bucket Lab: \n",
    "## A playground for understanding hydrologic process representations with Deep Learning\n",
    "\n",
    "This notebook is designed to help students understand the fundamentals of simulating dynamic systems with Deep Learning. \n",
    "\n",
    "\n",
    "The tutorial models a leaking bucket, a much simpler hydrological system than a watershed, allowing us to easily evaluate whether our leaking bucket models approximante expected physical processes.\n",
    "\n",
    "We generate all data synthetically in this notebook using simple numerical simulations of the hydrological system (leaking buckets) and varying bucket characteristics/attributes and variables including dynamic forcings for precipitation. \n",
    "\n",
    "We develop a single-layer Long Short-Term Memory (LSTM) network to learn dynamics of water level and fluxes out from bucket based on the \"ground truth\" data.\n",
    "\n",
    "In this notebook, the user can experiment with modifying the bucket model attributes and modelling setup to explore hydrologic process representations and their predictability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup\n",
    "\n",
    "The first thing to do is setup the notebook environment with all the libraries, declare model global parameters, settings, variables and functions that define the bucket system we want to represent, as well as define the hyperparameters and structure for the deep learning model. \n",
    "\n",
    "Note: in a typical full-scale modeling framework, these would be decalared in a configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importing libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import standard libraries for data management, calculations and plotting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import machine learning libraries for modeling architectures, training, validation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "from torch.autograd import Variable \n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Defining the physical bucket model system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global variables**\n",
    "\n",
    "Constants in our bucket model system are:\n",
    "- g, gravitational acceleration [$m/s^2$]\n",
    "- time step [$s$]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 9.807\n",
    "time_step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters for the Forcing Process (Precipitation)**\n",
    "\n",
    "To generate precipitation data for our hydrological system, we employ a simple random process that determines whether it is raining based on the previous state, as well as the total amount of rain during the event.\n",
    "\n",
    "We classify precipitation into three types: None, Light, and Heavy. Each type has distinct ranges of rainfall probabilities and depths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_probability_range = {\"None\": [0.6, 0.7], \n",
    "                          \"Light\": [0.5, 0.8], \n",
    "                          \"Heavy\": [0.2, 0.3]}\n",
    "\n",
    "rain_depth_range = {\"Light\": [0, 2], \"Heavy\": [2, 8]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bucket physical attributes**\n",
    "\n",
    "The physical leaky bucket attributes include:\n",
    "- A_bucket, bucket area [$m^2$]\n",
    "- A_spigot, spigot areas [$m^2$]\n",
    "- H_bucket, bucket heights [$m$]\n",
    "- H_spigot, spigot heights [$m$]\n",
    "- K_infiltration, infiltration rate [$mm/hr$]\n",
    "- ET_parameter, evapotranspiration parameter [$mm/day$]\n",
    "\n",
    "We will generate a diversity of leaky buckets by randomy selecting their physical attribute values from the following possible ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_attributes_range = {\"A_bucket\": [1.0, 2.0],\n",
    "                           \"A_spigot\": [0.1, 0.2],\n",
    "                           \"H_bucket\": [5.0, 6.0],\n",
    "                           \"H_spigot\": [1.0, 3.0],\n",
    "                           \"K_infiltration\": [1e-7, 1e-9],\n",
    "                           \"ET_parameter\": [7, 9]\n",
    "                          }\n",
    "\n",
    "bucket_attributes_list = list(bucket_attributes_range.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Channel physical parameters**\n",
    "\n",
    "This section includes the channel physical parameters. Initially, the transformation parameters are defined\n",
    "\n",
    "- transform_type: 0 (null transform), 1 (simple shift), 2 (time-based displacement), 3 (simple attenuation), 4 (LagK), 5 (Muskingum-Cunge)\n",
    "- shift_amount: position number to shift (applies when transform_type == 1)\n",
    "- time_shift: specified number of seconds to move the streamflow\n",
    "- attenuation_factor: value for multiplying the streamflow\n",
    "- lag_amount: number of streamflow positions to delay the output\n",
    "- param_list: list of parameters to apply Muskingum-Cunge transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_params_range = {\"transform_type\": [0, 1, 3, 4], #transform_types 2 and 5 are not currently implemented\n",
    "                          \"shift_amount\": [0, 100],\n",
    "                          # \"time_shift\": [],\n",
    "                          \"attenuation_factor\": [0.01, 0.99],\n",
    "                          \"lag_amount\": [0, 100]\n",
    "                         }\n",
    "\n",
    "transform_params_list = list(transform_params_range.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model input and output variables**\n",
    "\n",
    "The data fluxes in and out of the leaky bucket considered in this stystem and the leaky bucket state, include\n",
    "- precipitation into the bucket as a model input (precip) \n",
    "- the actual and potential loss to evaporation from the bucket as model inputs (et, pet)\n",
    "- the water flow over the bucket and out of the spigot as a simulated model output(q_overflow, q_spigot)\n",
    "- and the state of the water head in the bucket as a simluated model output (h_bucket).\n",
    "\n",
    "We define lists based on these input and output leaking bucket model variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vars = ['precip', 'et']\n",
    "input_vars.extend(bucket_attributes_list)\n",
    "output_vars = ['q_overflow', 'q_spigot']\n",
    "n_input = len(input_vars)\n",
    "n_output = len(output_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data noise**\n",
    "\n",
    "Because real world systems are noisy, we can add noise to the synthetic data by multiplying the values by a random factor taken from a normal distribution with a mean of 1 and a standard deviation prescribed for different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_noise = True\n",
    "noise = {\"pet\": 0.1, \"et\": 0.1, \"q\": 0.1, \"head\": 0.1} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustration of leaking bucket hydrological system\n",
    "\n",
    "\n",
    "![title](figs/bucket_schematic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Defining the modeling setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning Model (LSTM) Hyperparameters**\n",
    "\n",
    "The hyperparameters for the LSTM (Long Short-Term Memory) deep learning model include:\n",
    "- device: The device (CPU or CUDA) used for training and inference.\n",
    "- hidden_state_size: The number of units in the hidden state of the LSTM model.\n",
    "- num_layers: Number of LSTM layers.\n",
    "- num_epochs: Number of training epochs.\n",
    "- batch_size: Number of samples in each training batch.\n",
    "- seq_length: Length of input sequences.\n",
    "- learning_rate: Learning rate for the optimizer.\n",
    "- num_classes: Number of output classes.\n",
    "- input_size: Size of the input layer.\n",
    "\n",
    "These hyperparameters control the behavior and performance of the LSTM model and can be adjusted to optimize the model's accuracy and generalization capabilities for specific tasks and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Using CUDA device: \", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = 'cpu'\n",
    "hidden_state_size = 16\n",
    "num_layers = 8\n",
    "num_epochs = 5  \n",
    "batch_size = 256 \n",
    "seq_length = 24\n",
    "learning_rate = np.linspace(start=0.1, stop=0.01, num=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Size of data records and splits**\n",
    "\n",
    "The deep learning framework requires the data to be split into 3 different sets\n",
    "- Back propogation is done on the training set (train)\n",
    "- Hyperparameter tuning is done on the validation set (val)\n",
    "- Calculating model accuracy is done on the testing set (test)\n",
    "\n",
    "We will define how many randomly generated bucket configurations to use for the \"train\", \"val\", and \"test\" sets, as well as the length of the simulations for each set.\n",
    "\n",
    "For example, if we have a training set with 10 different buckets, the \"split\" refers to dividing the data into multiple bucket configurations. In this case, it means that we will train the model using 10 randomly generated bucket configurations.\n",
    "\n",
    "Furthermore, if we have 1000 time steps for each bucket, the \"time split\" indicates the number of time steps we will use for training the model. In this scenario, we will train the model using 1000 time steps for each of the 10 bucket configurations.\n",
    "\n",
    "**For creating a basin network**\n",
    "\n",
    "We will define three basin network sets for the use of \"train\", \"val\", and \"test\". Additionally it is necessary to define the number of buckets in every basin network (n_bucket_per_network). Consequently, the number of buckets is equal to the number of basin networks multiplied by the number of buckets per network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_buckets_split = {\"train\": 10, \"val\": 5,\"test\": 1}\n",
    "time_splits = {\"train\": 1000, \"val\": 500,\"test\": 500}\n",
    "\n",
    "n_buckets_per_network = 3\n",
    "n_basin_networks_split = {\"train\": 15, \"val\": 7, \"test\": 2}\n",
    "n_buckets_split = {\"train\": n_basin_networks_split[\"train\"] * n_buckets_per_network, \"val\": n_basin_networks_split[\"val\"] * n_buckets_per_network,\"test\": n_basin_networks_split[\"test\"] * n_buckets_per_network}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the settings above to calculate the total length of the data record to generate and the total number of buckets and basin networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = time_splits[\"train\"] + time_splits[\"val\"] + time_splits[\"test\"] + seq_length * 3\n",
    "n_buckets = n_buckets_split[\"train\"] + n_buckets_split[\"val\"] + n_buckets_split[\"test\"]\n",
    "n_basin_networks = n_basin_networks_split[\"train\"] + n_basin_networks_split[\"val\"] + n_basin_networks_split[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then calculate the parameters for the bucket and time splits necessary to feed the model with separate datasets for training, validation, and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a new function to calculate parameter for the basin network splits into separate datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_parameters():\n",
    "    # create lists of bucket indices for each set based on the given bucket splits\n",
    "    buckets_for_training = list(range(0, n_buckets_split['train'] + 1))\n",
    "    buckets_for_val = list(range(n_buckets_split['train'] + 1, \n",
    "                                 n_buckets_split['train'] + n_buckets_split['val'] + 1))\n",
    "    buckets_for_test = list(range(n_buckets - n_buckets_split['test'], n_buckets))\n",
    "\n",
    "    # determine the time range for each set based on the given time splits\n",
    "    train_start = seq_length\n",
    "    train_end   = time_splits[\"train\"]\n",
    "    val_start   = train_end + seq_length\n",
    "    val_end     = val_start + time_splits[\"val\"]\n",
    "    test_start  = val_end + seq_length\n",
    "    test_end    = test_start + time_splits[\"test\"]\n",
    "    \n",
    "    # organize the split parameters into separate lists for each set\n",
    "    train_split_parameters = [buckets_for_training, train_start, train_end]\n",
    "    val_split_parameters = [buckets_for_val, val_start, val_end]\n",
    "    test_split_parameters = [buckets_for_test, test_start, test_end]\n",
    "    \n",
    "    return [train_split_parameters, val_split_parameters, test_split_parameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45], 24, 1000], [[46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66], 1024, 1524], [[66, 67, 68, 69, 70, 71], 1548, 2048]]\n"
     ]
    }
   ],
   "source": [
    "[[buckets_for_training, train_start, train_end],\n",
    "[buckets_for_val, val_start, val_end],\n",
    "[buckets_for_test, test_start, test_end]]= split_parameters()\n",
    "print([[buckets_for_training, train_start, train_end],\n",
    "[buckets_for_val, val_start, val_end],\n",
    "[buckets_for_test, test_start, test_end]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will create separate datasets for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basin_split_param():\n",
    "    #create lists of basin indices for each set\n",
    "    basins_train = list(range(0, n_basin_networks_split['train']))\n",
    "    basins_val = list(range(n_basin_networks_split['train'], \n",
    "                            n_basin_networks_split['train'] + n_basin_networks_split['val']))\n",
    "    basins_test = list(range(n_basin_networks - n_basin_networks_split['test'], n_basin_networks)) \n",
    "    \n",
    "    #place basin split parameters into separate lists\n",
    "    train_basin_split_param = [basins_train]\n",
    "    val_basin_split_param = [basins_val]\n",
    "    test_basin_split_param = [basins_test]\n",
    "    \n",
    "    #determine transform parameters per basin network\n",
    "    \n",
    "    \n",
    "    return [train_basin_split_param, val_basin_split_param, test_basin_split_param]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will run the function above and print out the range for the whole basin network split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]], [[15, 16, 17, 18, 19, 20, 21]], [[22, 23]]]\n"
     ]
    }
   ],
   "source": [
    "[[basin_networks_train],[basin_networks_val],[basin_networks_test]]= basin_split_param()\n",
    "print([[basin_networks_train],[basin_networks_val],[basin_networks_test]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Creating a sample of diverse buckets\n",
    "\n",
    "Now that we know how many buckets we want, we can generate a sample of diverse buckets with their respective boundary and initial conditions by randomly sampling from the possible ranges for each attribute defined in the settings above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_buckets(n_buckets):\n",
    "    # Boundary conditions\n",
    "    buckets = {bucket_attribute:[] for bucket_attribute in bucket_attributes_list}\n",
    "    for i in range(n_buckets):\n",
    "        for attribute in bucket_attributes_list:\n",
    "            buckets[attribute].append(np.random.uniform(bucket_attributes_range[attribute][0], \n",
    "                                                        bucket_attributes_range[attribute][1]))\n",
    "\n",
    "    # Initial conditions\n",
    "    h_water_level = [np.random.random() for i in range(n_buckets)]\n",
    "    mass_overflow = [np.random.random() for i in range(n_buckets)]\n",
    "    return buckets, h_water_level, mass_overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets, h_water_level, mass_overflow = setup_buckets(n_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Creating the synthetic \"precipitation\" \n",
    "\n",
    "We first randomly assign rainfall parameters for each bucket, using the probability ranges specified for each precipitation type. We then generate synthetic input time series for each bucket model, known as forcing data. This process involves using the previously defined rainfall parameters and the random process defined in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_rain_params():\n",
    "    buck_rain_params = [rain_depth_range,\n",
    "                        np.random.uniform(rain_probability_range[\"None\"][0],\n",
    "                                            rain_probability_range[\"None\"][1]),\n",
    "                        np.random.uniform(rain_probability_range[\"Heavy\"][0],\n",
    "                                            rain_probability_range[\"Heavy\"][1]),\n",
    "                        np.random.uniform(rain_probability_range[\"Light\"][0],\n",
    "                                            rain_probability_range[\"Light\"][1])\n",
    "                 ]\n",
    "    return buck_rain_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rain(preceding_rain, bucket_rain_params):\n",
    "    depth_range, no_rain_probability, light_rain_probability, heavy_rain_probability = bucket_rain_params\n",
    "    # some percent of time we have no rain at all\n",
    "    if np.random.uniform(0.01, 0.99) < no_rain_probability:\n",
    "        rain = 0\n",
    "\n",
    "    # When we do have rain, the probability of heavy or light rain depends on the previous day's rainfall\n",
    "    else:\n",
    "        # If yesterday was a light rainy day, or no rain, then we are likely to have light rain today\n",
    "        if preceding_rain < depth_range[\"Light\"][1]:\n",
    "            if np.random.uniform(0, 1) < light_rain_probability:\n",
    "                rain = np.random.uniform(0, 1)\n",
    "            else:\n",
    "                # But if we do have heavy rain, then it could be very heavy\n",
    "                rain = np.random.uniform(depth_range[\"Heavy\"][0], depth_range[\"Heavy\"][1])\n",
    "\n",
    "        # If it was heavy rain yesterday, then we might have heavy rain again today\n",
    "        else:\n",
    "            if np.random.uniform(0, 1) < heavy_rain_probability:\n",
    "                rain = np.random.uniform(0, 1)\n",
    "            else:\n",
    "                rain = np.random.uniform(depth_range[\"Light\"][0], depth_range[\"Light\"][1])\n",
    "    return rain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a random rainfall input timeseries for each bucket and store it in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_list = {}\n",
    "for ibuc in range(n_buckets):\n",
    "    bucket_rain_params = pick_rain_params()\n",
    "    in_list[ibuc] = [0]\n",
    "    for i in range(1, num_records):\n",
    "        in_list[ibuc].append(random_rain(in_list[ibuc][i-1], bucket_rain_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Running Numerical Simulations of the Bucket Model to Generate \"Ground Truth\" Data\n",
    "\n",
    "We run bucket model simulations to generate the data for our system. \n",
    "\n",
    "These data represent the \"ground truth\", which is what we will try to learn with the LSTM. \n",
    "\n",
    "We perform numerical simulations of a bucket model for a specific bucket index. It iterates over each time step, updating the water level based on precipitation, evapotranspiration, infiltration, overflow, and spigot outflow. The simulation results, including the water level, overflow, spigot flow, and other attributes, are stored in a data frame. This is a concise representation of the bucket model's behavior over time for the given bucket index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bucket_simulation(ibuc):\n",
    "    columns = ['precip', 'et', 'h_bucket', 'q_overflow', 'q_spigot']\n",
    "    columns.extend(bucket_attributes_list)\n",
    "    # Memory to store model results\n",
    "    df = pd.DataFrame(index=list(range(len(in_list[ibuc]))), columns=columns)\n",
    "    #print(list(range(len(in_list[ibuc]))))\n",
    "    # Main loop through time\n",
    "    for t, precip_in in enumerate(in_list[ibuc]):\n",
    "        \n",
    "        # Add the input mass to the bucket\n",
    "        h_water_level[ibuc] = h_water_level[ibuc] + precip_in\n",
    "\n",
    "        # Lose mass out of the bucket. Some periodic type loss, evaporation, and some infiltration...\n",
    "        et = np.max([0, (buckets[\"A_bucket\"][ibuc] / buckets[\"ET_parameter\"][ibuc]) * np.sin(t) * np.random.normal(1, noise['pet'])])\n",
    "        infiltration = h_water_level[ibuc] * buckets[\"K_infiltration\"][ibuc]\n",
    "        h_water_level[ibuc] = np.max([0 , (h_water_level[ibuc] - et)])\n",
    "        h_water_level[ibuc] = np.max([0 , (h_water_level[ibuc] - infiltration)])\n",
    "        if is_noise:\n",
    "            h_water_level[ibuc] = h_water_level[ibuc] * np.random.normal(1, noise['et'])\n",
    "\n",
    "        # Overflow if the bucket is too full\n",
    "        if h_water_level[ibuc] > buckets[\"H_bucket\"][ibuc]:\n",
    "            mass_overflow[ibuc] = h_water_level[ibuc] - buckets[\"H_bucket\"][ibuc]\n",
    "            h_water_level[ibuc] = buckets[\"H_bucket\"][ibuc] \n",
    "            if is_noise:\n",
    "                h_water_level[ibuc] = h_water_level[ibuc] - np.random.uniform(0, noise['q'])\n",
    "\n",
    "        # Calculate head on the spigot\n",
    "        h_head_over_spigot = (h_water_level[ibuc] - buckets[\"H_spigot\"][ibuc] ) \n",
    "        if is_noise:\n",
    "            h_head_over_spigot = h_head_over_spigot * np.random.normal(1, noise['head'])\n",
    "\n",
    "        # Calculate water leaving bucket through spigot\n",
    "        if h_head_over_spigot > 0:\n",
    "            velocity_out = np.sqrt(2 * g * h_head_over_spigot)\n",
    "            spigot_out = velocity_out *  buckets[\"A_spigot\"][ibuc] * time_step\n",
    "            h_water_level[ibuc] = h_water_level[ibuc] - spigot_out\n",
    "        else:\n",
    "            spigot_out = 0\n",
    "\n",
    "        # Save the data in time series\n",
    "        df.loc[t,'precip'] = precip_in\n",
    "        df.loc[t,'et'] = et\n",
    "        df.loc[t,'h_bucket'] = h_water_level[ibuc]\n",
    "        df.loc[t,'q_overflow'] = mass_overflow[ibuc]\n",
    "        df.loc[t,'q_spigot'] = spigot_out\n",
    "        for attribute in bucket_attributes_list:\n",
    "            df.loc[t, attribute] = buckets[attribute][ibuc]\n",
    "\n",
    "        mass_overflow[ibuc] = 0\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we run and store the simulations for each bucket in a dictionnary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran basin network model simulation to create a dictionary that contains n_basin_networks elements. Each element represents a network of n_buckets_per_network buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_basin_network_simultation():\n",
    "    result = {}\n",
    "    result['buckets'] = {}\n",
    "    for i in range(n_basin_networks):\n",
    "        bucket_dictionary = {}\n",
    "        for j in range(n_buckets_per_network):\n",
    "            bucket_dictionary[j] = run_bucket_simulation((i * n_buckets_per_network) + j)\n",
    "        result['buckets'][i] = bucket_dictionary\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "basin_network_dictionary = {}\n",
    "basin_network_dictionary = run_basin_network_simultation()\n",
    "#print(basin_network_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_q_bucket(bucket_df):\n",
    "    return bucket_df['q_overflow']+bucket_df['q_spigot']\n",
    "\n",
    "def sum_q_network(network_dict):\n",
    "    for k, bucket in network_dict.items():\n",
    "        bucket['q_total']=sum_q_bucket(bucket)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, network in basin_network_dictionary['buckets'].items():\n",
    "    sum_q_network(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(basin_network_dictionary)\n",
    "#df = pd.DataFrame.from_dict(basin_network_dictionary)\n",
    "#df.to_csv('basin_network_data.csv', index=False, header=True)\n",
    "\n",
    "#df1 = pd.DataFrame.from_dict(basin_network_dictionary['buckets'][0][2])\n",
    "#df1.to_csv('buckets.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the simulation results for the first basin network (index 0)\n",
    "basin_network_index = 3\n",
    "bucket_results = basin_network_dictionary['buckets'][basin_network_index]\n",
    "# Plot the simulation results for each bucket in the basin network\n",
    "for bucket_index, df in bucket_results.items():\n",
    "    time_steps = range(len(df))\n",
    "    # Plot the water level in the bucket\n",
    "    plt.plot(time_steps, df['h_bucket'], label=f'Bucket {bucket_index}' )\n",
    "# Add labels and legend\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Water Level')\n",
    "plt.title(f'Simulation Results for Basin Network {basin_network_index}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# result = run_basin_network_simultation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# plot_diagram(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result['buckets'][3][0]['q_total']= result['buckets'][3][0]['q_overflow']+result['buckets'][3][0]['q_spigot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresult\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuckets\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m3\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    " #result['buckets'][3][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.7 Create a combined streamflow\n",
    "\n",
    "We create a function to transform streamflow. The transformation type to perform can be one of the following:\n",
    "\n",
    "- **Null transformation:** Returns the same series of input streamflows\n",
    "- **Simple shift:** Move the elements right or left according to parameters shift_amount\n",
    "- **Time-based shift:** Move the elements to the right or left according to the specified number of seconds time_shift; requires some information about the internal time values of the series.\n",
    "- **Simple Attenuation:** multiply each element of the series by an attenuation factor\n",
    "- **LagK:** Move each element to position K (lag_amount)\n",
    "- **Muskingum-Cunge:** Muskingum-Cunge method to combine river flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def mcTransform(Q, param_list):\n",
    "#    return Q\n",
    "\n",
    "def transformQ(Q, T):\n",
    "    # Verify if the transformation parameters are null\n",
    "    if T[0] == 0 or T[0] is None:\n",
    "        # The null transformation returns the same series of input streamflows\n",
    "        return Q\n",
    "    else:\n",
    "        transform_type = T[0]\n",
    "        if transform_type == 1:\n",
    "            # Implement the simple shift transformation\n",
    "            # Move the elements to the right or left according to parameters\n",
    "            shift_amount = T[1]\n",
    "            #print(shift_amount)\n",
    "            #print(Q[shift_amount:])\n",
    "            #print(Q[:shift_amount])\n",
    "            #transformed_Q = Q[shift_amount:] + Q[:shift_amount]\n",
    "            transformed_Q = np.roll(Q, shift_amount).tolist()\n",
    "        elif transform_type == 2:\n",
    "            # Implementing time-based displacement transformation\n",
    "            # Move the elements to the right or left according to the specified number of seconds\n",
    "            time_shift = T[1]\n",
    "            # Information about the internal time values of the series should be used here\n",
    "            # to calculate the appropriate displacement based on the specified seconds\n",
    "            # transformed_Q = ...\n",
    "            # Implement the code for this transformation according to needs and available data\n",
    "        elif transform_type == 3:\n",
    "            # Implement the simple attenuation transformation\n",
    "            # Multiplying each element of the series by an attenuation factor\n",
    "            attenuation_factor = T[1]\n",
    "            transformed_Q = [q * attenuation_factor for q in Q]\n",
    "        elif transform_type == 4:\n",
    "            # Implement LagK transformation\n",
    "            # Move each element to position K\n",
    "            lag_amount = T[1]\n",
    "            transformed_Q = [Q[i - lag_amount] if i >= lag_amount else 0 for i in range(len(Q))]\n",
    "        elif transform_type == 5:\n",
    "            # Implement the Muskingum-Cunge method to combine river flows\n",
    "            param_list = T[1]\n",
    "            # transformed_Q = mcTransform(Q, param_list)\n",
    "            # Implement the code for this transformation according to needs and available data\n",
    "        #else:\n",
    "            # In case new transformations are added in the future, they can be handled here\n",
    "            # by adding more elif blocks\n",
    "        return transformed_Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we combine the transformed streamflows of a basin network in one single streamflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_key(transform_type):\n",
    "    if transform_type == 0:\n",
    "        return None\n",
    "    elif transform_type == 1:\n",
    "        return \"shift_amount\"\n",
    "    #elif transform_type == 2:  # For time-based transformation (not implemented yet)\n",
    "    #    return \"time_shift\"\n",
    "    elif transform_type == 3:\n",
    "        return \"attenuation_factor\"\n",
    "    elif transform_type == 4:\n",
    "        return \"lag_amount\"\n",
    "    #elif tranform_type == 5:  # For Muskingum-Cunge transformation (not implemented yet)\n",
    "    #    return \"param_list\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transform_params(bucket_basin):\n",
    "    n = len(bucket_basin)\n",
    "    transform_params = []\n",
    "    for i in range(n - 1):\n",
    "        transform_type = np.random.choice(transform_params_range[\"transform_type\"])\n",
    "        param_key = get_param_key(transform_type)\n",
    "        if transform_type == 0:\n",
    "            param_value = None\n",
    "        else:\n",
    "            param_range = transform_params_range[param_key]\n",
    "            #param_value = None\n",
    "            if param_key in [\"shift_amount\", \"lag_amount\"]:\n",
    "                param_value = np.random.randint(param_range[0], param_range[1] + 1)\n",
    "            else:\n",
    "                param_value = np.random.uniform(param_range[0], param_range[1])\n",
    "        transform_params.append((transform_type, param_value))\n",
    "    transform_params.append((0, None))\n",
    "    return transform_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transform_params_network():\n",
    "    transform_params_vector = []\n",
    "    for i in range(n_basin_networks):\n",
    "        transform_params_vector.append([i, generate_transform_params(basin_network_dictionary['buckets'][i])])\n",
    "    return transform_params_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_params_vector = generate_transform_params_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, [(4, 29), (3, 0.4574626099854994), (0, None)]], [1, [(4, 27), (3, 0.44019956014266015), (0, None)]], [2, [(1, 28), (1, 73), (0, None)]], [3, [(1, 14), (1, 69), (0, None)]], [4, [(4, 27), (4, 96), (0, None)]], [5, [(0, None), (3, 0.10604508323484506), (0, None)]], [6, [(1, 22), (3, 0.3418292640751636), (0, None)]], [7, [(1, 60), (3, 0.7146061388710315), (0, None)]], [8, [(0, None), (1, 20), (0, None)]], [9, [(3, 0.3544260221983905), (3, 0.5638987641891546), (0, None)]], [10, [(4, 13), (4, 65), (0, None)]], [11, [(4, 48), (3, 0.6776561821506532), (0, None)]], [12, [(0, None), (4, 75), (0, None)]], [13, [(1, 62), (3, 0.5587372555372387), (0, None)]], [14, [(3, 0.12564078298948353), (3, 0.6595240974511546), (0, None)]], [15, [(4, 1), (1, 42), (0, None)]], [16, [(0, None), (1, 83), (0, None)]], [17, [(4, 58), (3, 0.5273495111924171), (0, None)]], [18, [(0, None), (1, 11), (0, None)]], [19, [(3, 0.7673139886963963), (3, 0.14906103307046048), (0, None)]], [20, [(0, None), (0, None), (0, None)]], [21, [(3, 0.18341125877983527), (1, 64), (0, None)]], [22, [(4, 33), (0, None), (0, None)]], [23, [(4, 14), (0, None), (0, None)]]]\n"
     ]
    }
   ],
   "source": [
    "print(transform_params_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_streamflows(flows, transform_params):\n",
    "    transformed_flows = []\n",
    "    for Q, T in zip(flows, transform_params):\n",
    "        transformed_flows.append(transformQ(Q, T))\n",
    "    '''\n",
    "    print(\"Flows:\")\n",
    "    print(flows)\n",
    "    print(\"Transform_params:\")\n",
    "    print(transform_params)\n",
    "    print(\"Transformed_Flows:\")\n",
    "    print(transformed_flows)\n",
    "    print(\"suma:\")\n",
    "    print(np.sum(transformed_flows, axis=0))\n",
    "    '''\n",
    "    return np.sum(transformed_flows, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def streamflows_output(network_collection):\n",
    "    combined_streamflows_collection = {}\n",
    "    for i, (k, network) in enumerate(network_collection.items()):  # We should iterate on objects consistently -- this is messy.\n",
    "        q_total_vector = []\n",
    "        for bucket in network.values():\n",
    "            # print(bucket['q_total'])\n",
    "            q_total_vector.append(bucket['q_total'])\n",
    "        combined_streamflows_collection[k] = {}\n",
    "        combined_streamflows_collection[k]['q_total_output'] = combine_streamflows(q_total_vector, transform_params_vector[i][1])\n",
    "    return combined_streamflows_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_streamflows_dictionary = streamflows_output(basin_network_dictionary['buckets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_streamflows_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q1 = [0.4, 0.5, 0.6]\n",
    "Q2 = [0.1, 0.2, 0.3]\n",
    "Q3 = [0.7, 0.8, 0.9]\n",
    "Q4 = [1.0, 1.1, 1.2]\n",
    "\n",
    "flows = [Q1, Q2, Q3, Q4]\n",
    "\n",
    "shift_amount = 2\n",
    "attenuation_factor = 0.5\n",
    "lag_amount = 2\n",
    "\n",
    "transform_params = [(0, None), (1, shift_amount), (3, attenuation_factor), (4, lag_amount)]\n",
    "\n",
    "\n",
    "print(combine_streamflows(flows, transform_params))\n",
    "\n",
    "#print(basin_network_dictionary['buckets'][0][1])\n",
    "print(basin_network_dictionary['buckets'][0][0]['q_overflow'])\n",
    "print(basin_network_dictionary['buckets'][0][1]['q_overflow'])\n",
    "\n",
    "#print(basin_network_dictionary['buckets'][0][1]['q_spigot'])\n",
    "print(combine_streamflows([basin_network_dictionary['buckets'][0][0]['q_overflow'], basin_network_dictionary['buckets'][0][1]['q_overflow']], [(0, None), (0, None)]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.8 Visualizing a sample of the bucket fluxes\n",
    "We plot the model simulations to ensure the existence of fluxes and validate that the generated values are realistics for both the spigot (channel flow) and over the top (flooding). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_simulation(ibuc):\n",
    "    fig = plt.figure(figsize=(12, 3))\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    print('Bucket:', ibuc)\n",
    "    print(\"Overflow mean:\", np.round(bucket_dictionary[ibuc].q_overflow.mean(),2))\n",
    "    print(\"Overflow max:\", np.round(bucket_dictionary[ibuc].q_overflow.max(),2))\n",
    "    bucket_dictionary[ibuc].loc[:100,input_vars].plot(ax=ax1)\n",
    "    bucket_dictionary[ibuc].loc[:100,output_vars].plot(ax=ax2)\n",
    "    bucket_dictionary[ibuc].loc[:100,\"h_bucket\"].plot(ax=ax2)\n",
    "    ax1.set_title('Model inputs')\n",
    "    ax2.set_title('Model outputs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ibuc in buckets_for_val:\n",
    "    viz_simulation(ibuc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Deep learning model\n",
    "This section sets up our deep learning model and training procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Defining the neural network model\n",
    "This is the part of the notebook where we will be using the previous simulations to learn from the generated data and subsequently make predictions. Here, we leverage the simulations to extract valuable insights and apply them in our learning and prediction processes.\n",
    "\n",
    "\n",
    "Here we define a class called LSTM1, which is a PyTorch module for a single-layer Long Short-Term Memory (LSTM) network. \n",
    "\n",
    "**Brief explanation**\n",
    "- The input to the module is a tensor x of shape (batch_size, seq_length, input_size), which represents a sequence of batch_size samples, each of length seq_length, with input_size features at each time step. \n",
    "- The LSTM layer is defined using the nn.LSTM class, with input_size as the size of the input layer, hidden_size as the size of the hidden state, and batch_first=True indicating that the first dimension of the input tensor is the batch size. \n",
    "- The output of the LSTM layer is passed through a ReLU activation function, and then to a fully connected layer (nn.Linear) with num_classes output units. \n",
    "- The forward method takes the input tensor x as an argument, along with an optional tuple init_states representing the initial hidden and internal states of the LSTM layer, and returns the output tensor prediction. \n",
    "- If init_states is not provided, it is initialized as a tensor of zeros with shape (batch_size, hidden_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, batch_size, seq_length):\n",
    "        super(LSTM1, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc_1 =  nn.Linear(hidden_size, num_classes) #fully connected 1\n",
    "   \n",
    "    def forward(self, x, init_states=None):\n",
    "\n",
    "        if init_states is None:\n",
    "            h_t = Variable(torch.zeros(batch_size, self.hidden_size)) # hidden state\n",
    "            c_t = Variable(torch.zeros(batch_size, self.hidden_size)) # internal state\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "           \n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.relu(out)\n",
    "        prediction = self.fc_1(out) # Dense, fully connected layer\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Defining a procedure for model validation\n",
    "Here we verify that our model is working the way we expect. We would especially want to check model validation when changing hyperparameters.\n",
    "\n",
    "We define a function that validates and tests the LSTM model, as well as checks the water balance of the system. \n",
    "\n",
    "**Brief explanation**\n",
    "- We use the pre-defined LSTM model to make predictions on the validation data. \n",
    "- The output of this model is then used to compute two different metrics, the Nash-Sutcliffe Efficiency (NSE) for the spigot_out and mass_overflow columns of the dataframe.\n",
    "- We plot the actual spigot_out and mass_overflow values against their corresponding LSTM predictions. \n",
    "- We check the water balance of the system by summing up the input, evapotranspiration, mass_overflow, spigot_out, and the last recorded water level in the dataframe, and compare this to the total mass out of or left in the system. \n",
    "- We print out the percent mass residual as a measure of how well the system is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_validation_period(lstm, np_val_seq_X, ibuc, n_plot=100):\n",
    "    \n",
    "    def __make_prediction():\n",
    "        lstm_output_val = lstm(torch.Tensor(np_val_seq_X[ibuc]).to(device=device))\n",
    "        val_spigot_prediction = []\n",
    "        val_overflow_prediction = []\n",
    "        for i in range(lstm_output_val.shape[0]):\n",
    "            val_spigot_prediction.append((lstm_output_val[i,-1,1].cpu().detach().numpy() * \\\n",
    "                                    np.std(df.loc[train_start:train_end,'q_spigot'])) + \\\n",
    "                                   np.mean(df.loc[train_start:train_end,'q_spigot']))\n",
    "\n",
    "            val_overflow_prediction.append((lstm_output_val[i,-1,0].cpu().detach().numpy() * \\\n",
    "                                    np.std(df.loc[train_start:train_end,'q_overflow'])) + \\\n",
    "                                   np.mean(df.loc[train_start:train_end,'q_overflow']))\n",
    "        return val_spigot_prediction, val_overflow_prediction\n",
    "    \n",
    "    def __compute_nse():\n",
    "        spigot_out = df.loc[val_start:val_end, 'q_spigot']\n",
    "        spigot_mean = np.mean(spigot_out)\n",
    "        spigot_pred_variance = 0\n",
    "        spigot_obs_variance = 0\n",
    "\n",
    "        overflow_out = df.loc[val_start:val_end, 'q_overflow']\n",
    "        overflow_mean = np.mean(overflow_out)\n",
    "        overflow_pred_variance = 0\n",
    "        overflow_obs_variance = 0\n",
    "\n",
    "        for i, pred_spigot in enumerate(val_spigot_prediction):\n",
    "            t = i + seq_length - 1\n",
    "            spigot_pred_variance += np.power(( pred_spigot          - spigot_out.values[t]), 2)\n",
    "            spigot_obs_variance  += np.power(( spigot_mean          - spigot_out.values[t]), 2)\n",
    "\n",
    "        for i, pred_overflow in enumerate(val_overflow_prediction):\n",
    "            t = i + seq_length - 1\n",
    "            overflow_pred_variance += np.power((pred_overflow          - overflow_out.values[t]), 2)\n",
    "            overflow_obs_variance  += np.power((overflow_mean          - overflow_out.values[t]), 2)\n",
    "        spigot_nse = np.round( 1 - ( spigot_pred_variance / spigot_obs_variance   ), 4)\n",
    "        overland_flow_nse = np.round( 1 - ( overflow_pred_variance / overflow_obs_variance ), 4)\n",
    "        return spigot_nse, overland_flow_nse\n",
    "\n",
    "    def __compute_mass_balance():\n",
    "        mass_in = df.sum()['precip']\n",
    "        mass_out = df.sum()['et'] + \\\n",
    "                   df.sum()['q_overflow'] + \\\n",
    "                   df.sum()['q_spigot'] + \\\n",
    "                   df.loc[num_records - 1, 'h_bucket']\n",
    "        return mass_in, mass_out\n",
    "\n",
    "        \n",
    "    df = bucket_dictionary[ibuc]\n",
    "    val_spigot_prediction, val_overflow_prediction = __make_prediction()\n",
    "    spigot_nse, overland_flow_nse = __compute_nse()\n",
    "    mass_in, mass_out = __compute_mass_balance()\n",
    "        \n",
    "    print(\"Spigot NSE\", spigot_nse)\n",
    "    print(\"Overflow NSE\", overland_flow_nse)\n",
    "    print(\"Mass into the system: \", mass_in)\n",
    "    print(\"Mass out or left over:\", mass_out)\n",
    "    print(\"percent mass resudual: {:.0%}\".format((mass_in - mass_out) /mass_in))\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 3))\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    ax1.plot(df.loc[val_start + seq_length-1:val_start+n_plot+seq_length - 1,'q_spigot'].values, label=\"Spigot out\")\n",
    "    ax1.plot(val_spigot_prediction[:n_plot], label=\"LSTM spigot out\")\n",
    "    ax1.legend()\n",
    "    ax2.plot(df.loc[val_start+seq_length-1:val_start+n_plot+seq_length-1,'q_overflow'].values, label=\"Overflow\")\n",
    "    ax2.plot(val_overflow_prediction[:n_plot], label=\"LSTM Overflow\")\n",
    "    ax2.legend()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Instantiating the neural network model (LSTM)\n",
    "\n",
    "Using the hyperparameters from Section 1.2, we define a specific instance of the LSTM model and set up the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "lstm = LSTM1(num_classes=n_output,  \n",
    "             input_size=n_input,    \n",
    "             hidden_size=hidden_state_size, \n",
    "             num_layers=num_layers, \n",
    "             batch_size=batch_size, \n",
    "             seq_length=seq_length).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Setting up the data to feed into the model\n",
    "\n",
    "We will set up data for:\n",
    "- training, to calculate the loss which is backpropogated through the model\n",
    "- validation, where we get predictions from the trained model and see if the performance is up to our standards\n",
    "- testing, the data we will utimately use to report the LSTM performance. \n",
    "\n",
    "Note: testing is the last thing we would do, if we go back to validation after this step, we would be P-hacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting a scaler to the training set to transform all the data**\n",
    "\n",
    "Here we fit a scaler to the training set, allowing for the transformation of input and output variables to a normalized and standardized scale, which helps in training the model and maintaining consistency across different datasets. This normalization step ensures that the data is suitable for training our LSTM model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_scaler():\n",
    "    frames = [bucket_dictionary[ibuc].loc[train_start:train_end, input_vars] for ibuc in buckets_for_training]\n",
    "    df_in = pd.concat(frames)    \n",
    "    scaler_in = StandardScaler()\n",
    "    scaler_train_in = scaler_in.fit_transform(df_in)\n",
    "\n",
    "    frames = [bucket_dictionary[ibuc].loc[train_start:train_end, output_vars] for ibuc in buckets_for_training]\n",
    "    df_out = pd.concat(frames)    \n",
    "    scaler_out = StandardScaler()\n",
    "    scaler_train_out = scaler_out.fit_transform(df_out)\n",
    "    return scaler_in, scaler_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_in, scaler_out = fit_scaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to create data loader for each data split**\n",
    "\n",
    "We prepare and organize the data for training. We create data loaders that handle batch processing and shuffling of the data. We also add some preprocessing steps such as scaling the input and output variables. The data loaders and numpy arrays are used for feeding the data into the neural network during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_loader(start, end, bucket_list):\n",
    "    loader = {}\n",
    "    np_seq_X = {}\n",
    "    np_seq_y = {}\n",
    "    for ibuc in bucket_list:\n",
    "        df = bucket_dictionary[ibuc]\n",
    "        scaler_in_i = scaler_in.transform(df.loc[start:end, input_vars])\n",
    "        scaler_out_i = scaler_out.transform(df.loc[start:end, output_vars])\n",
    "        np_seq_X[ibuc] = np.zeros((scaler_in_i.shape[0] - seq_length, seq_length, n_input))\n",
    "        np_seq_y[ibuc] = np.zeros((scaler_out_i.shape[0] - seq_length, seq_length, n_output))\n",
    "        for i in range(0, scaler_in_i.shape[0] - seq_length):\n",
    "            t = i+seq_length\n",
    "            np_seq_X[ibuc][i, :, :] = scaler_in_i[i:t,:]\n",
    "            np_seq_y[ibuc][i, :, :] = scaler_out_i[i:t,:]\n",
    "\n",
    "        ds = torch.utils.data.TensorDataset(torch.Tensor(np_seq_X[ibuc]), \n",
    "                                                  torch.Tensor(np_seq_y[ibuc]))\n",
    "        loader[ibuc] = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "    return loader, np_seq_X, np_seq_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the function above and parameters defined in the notebook environment to generate the training, validation, test data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, np_train_seq_X, np_train_seq_y = make_data_loader(train_start, train_end, buckets_for_training)\n",
    "val_loader, np_val_seq_X, np_val_seq_y = make_data_loader(val_start, val_end, buckets_for_val)\n",
    "test_loader, np_test_seq_X, np_test_seq_y = make_data_loader(test_start, test_end, buckets_for_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Training the model: Learning the general response of the example dynamic ''hydrologic\" system\n",
    "Now is the time to train the model! Everything above was done in preparation for this step.\n",
    "\n",
    "Here we define a function to train the LSTM neural network model with the nn.MSELoss() loss function and using the Adam optimizer and hyperparameters defined above.\n",
    "\n",
    "**Brief explanation**:\n",
    "- The training is done for a specified number of epochs.\n",
    "- For each epoch, the training data is divided into buckets. \n",
    "- For each bucket, the data is loaded using a PyTorch DataLoader and passed through the LSTM model. \n",
    "- The output is then compared with the target values using the custom loss function. \n",
    "- The gradients are calculated and the optimizer is used to update the weights of the model. \n",
    "- We use the tqdm library to show the progress of the training. \n",
    "- Finally, we estimate the average RMSE for each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(lstm, train_loader, buckets_for_training):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(lstm.parameters(), lr=learning_rate[0])\n",
    "    epoch_bar = tqdm(range(num_epochs),desc=\"Training\", position=0, total=num_epochs)\n",
    "    \n",
    "    # Create a dictionary to store the results\n",
    "    results = {}\n",
    "    for epoch in epoch_bar:\n",
    "\n",
    "        for ibuc in buckets_for_training:\n",
    "\n",
    "            batch_bar = tqdm(enumerate(train_loader[ibuc]),\n",
    "                             desc=\"Bucket: {}, Epoch: {}\".format(str(ibuc),str(epoch)),\n",
    "                             position=1,\n",
    "                             total=len(train_loader[ibuc]), leave=False, disable=True)\n",
    "\n",
    "            for i, (data, targets) in batch_bar:\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                optimizer = optim.Adam(lstm.parameters(), lr=learning_rate[epoch])\n",
    "\n",
    "                data = data.to(device=device)\n",
    "                targets = targets.to(device=device)\n",
    "\n",
    "                # Forward\n",
    "                lstm_output = lstm(data) \n",
    "                loss = criterion(lstm_output, targets)\n",
    "\n",
    "                #backward\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # gradient descent or adam step\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_bar.set_postfix(loss=loss.to(device).item(),\n",
    "                                      RMSE=\"{:.2f}\".format(loss**(1/2)),\n",
    "                                      epoch=epoch)\n",
    "                batch_bar.update()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                rmse_list = []\n",
    "                for i, (data_, targets_) in enumerate(train_loader[ibuc]):\n",
    "                    data_ = data_.to(device=device)\n",
    "                    targets_ = targets_.to(device=device)\n",
    "                    lstm_output_ = lstm(data_)\n",
    "                    MSE_ = criterion(lstm_output_, targets_)\n",
    "                    rmse_list.append(MSE_**(1/2))\n",
    "\n",
    "            meanrmse = np.mean(np.array(torch.Tensor(rmse_list)))\n",
    "            epoch_bar.set_postfix(loss=loss.cpu().item(),\n",
    "                                  RMSE=\"{:.2f}\".format(meanrmse),\n",
    "                                  epoch=epoch)\n",
    "            \n",
    "            if ibuc not in results:\n",
    "                results[ibuc] = {\"loss\": [], \"RMSE\": []}\n",
    "            results[ibuc][\"loss\"].append(loss.cpu().item())\n",
    "            results[ibuc][\"RMSE\"].append(meanrmse)\n",
    "            #....todo..? also add IT metrics in results in this function ...\n",
    "            batch_bar.update()\n",
    "\n",
    "        𝗰𝗹𝗲𝗮𝗿_𝗼𝘂𝘁𝗽𝘂𝘁\n",
    "        \n",
    "    return lstm, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the train model functon by prescribing the buckets and data to use for training and the intantiated LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm, results = train_model(lstm, train_loader, buckets_for_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Visualizing the learning curves\n",
    "\n",
    " We plot the loss and root mean square error (RMSE) metrics for each epoch to check if the model fitting has converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_learning_curve(results):\n",
    "    fig = plt.figure(figsize=(12, 3))\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    for ibuc in buckets_for_training:\n",
    "        ax1.plot(range(num_epochs), results[ibuc]['loss'])\n",
    "        ax2.plot(range(num_epochs), results[ibuc]['RMSE'])\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax2.set_ylabel('RMSE')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    plt.suptitle(\"Learning curves for each bucket\") \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_learning_curve(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Checking that the model works on the validation data\n",
    "Now that we have a trained model, we can see how it works on our validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ibuc in buckets_for_val:\n",
    "    check_validation_period(lstm, np_val_seq_X, ibuc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Experimentation\n",
    "\n",
    "Now that we have a general leaky bucket model setup and training/validation framework, we can use it to experiment with variations on the bucket characteristics and modelling setup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing some bucket configuration attributes here to modify the \"ground truth\" data, for example:\n",
    "- to simulate a more \"flashy\" system, reduce the probability of heavy precipitation and increase the magnitude\n",
    "- to simulate smaller buckets, reduce the size of the bucket attributes\n",
    "- or to add more noise, increase noise from 0.1 to different values \n",
    "\n",
    "Try changing some aspects of the modelling setup, for example:\n",
    "- increase or decrease the number of training buckets\n",
    "- increase or decrease the timeseries length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_probability_range = {\"None\": [0.6, 0.7], \"Light\": [0.5, 0.8], \"Heavy\": [0.05, 0.1]}\n",
    "rain_depth_range = {\"Light\": [0, 2], \"Heavy\": [6, 14]}\n",
    "\n",
    "bucket_attributes_range = {\"A_bucket\": [1.0, 2.0],\n",
    "                           \"A_spigot\": [0.1, 0.2],\n",
    "                           \"H_bucket\": [5.0, 6.0],\n",
    "                           \"H_spigot\": [1.0, 3.0],\n",
    "                           \"K_infiltration\": [1e-7, 1e-9],\n",
    "                           \"ET_parameter\": [7, 9]\n",
    "                          }\n",
    "\n",
    "noise = {\"pet\": 0.1, \"et\": 0.1, \"q\": 0.1, \"head\": 0.1} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_buckets_split = {\"train\": 25, \"val\": 5,\"test\": 1}\n",
    "time_splits = {\"train\": 2000, \"val\": 1000,\"test\": 500}\n",
    "num_records = time_splits[\"train\"] + time_splits[\"val\"] + time_splits[\"test\"] + seq_length * 3\n",
    "n_buckets = n_buckets_split[\"train\"] + n_buckets_split[\"val\"] + n_buckets_split[\"test\"]\n",
    "\n",
    "[[buckets_for_training, train_start, train_end],\n",
    "[buckets_for_val, val_start, val_end],\n",
    "[buckets_for_test, test_start, test_end]]= split_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-generate forcings and 'ground truth' data for buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "buckets, h_water_level, mass_overflow = setup_buckets(n_buckets)\n",
    "\n",
    "in_list = {}\n",
    "for ibuc in range(n_buckets):\n",
    "    bucket_rain_params = pick_rain_params()\n",
    "    in_list[ibuc] = [0]\n",
    "    for i in range(1, num_records):\n",
    "        in_list[ibuc].append(random_rain(in_list[ibuc][i-1], bucket_rain_params))\n",
    "\n",
    "bucket_dictionary = {}\n",
    "for ibuc in range(n_buckets):\n",
    "    bucket_dictionary[ibuc] = run_bucket_simulation(ibuc)\n",
    "    \n",
    "for ibuc in buckets_for_val:\n",
    "    viz_simulation(ibuc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-create new data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_in, scaler_out = fit_scaler()\n",
    "train_loader, np_train_seq_X, np_train_seq_y = make_data_loader(train_start, train_end, buckets_for_training)\n",
    "val_loader, np_val_seq_X, np_val_seq_y = make_data_loader(val_start, val_end, buckets_for_val)\n",
    "test_loader, np_test_seq_X, np_test_seq_y = make_data_loader(test_start, test_end, buckets_for_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinitializing the LSTM model, the training process is performed with the updated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM1(num_classes=n_output,  \n",
    "             input_size=n_input,    \n",
    "             hidden_size=hidden_state_size, \n",
    "             num_layers=num_layers, \n",
    "             batch_size=batch_size, \n",
    "             seq_length=seq_length).to(device=device)\n",
    "\n",
    "lstm, results = train_model(lstm, train_loader, buckets_for_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the learning curve based on the training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "viz_learning_curve(results)\n",
    "\n",
    "for ibuc in buckets_for_val:\n",
    "    check_validation_period(lstm, np_val_seq_X, ibuc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
